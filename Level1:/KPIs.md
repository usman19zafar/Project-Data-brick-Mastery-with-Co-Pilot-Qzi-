MODULE 1.1 â€” Workspace & Notebooks
Purpose
Establish a working Databricks environment and basic notebook fluency.

Deliverables
One Python notebook

One SQL notebook

Markdown + code + results in a single file

KPIs
KPIâ€‘1: User can create and run notebooks without cluster errors

KPIâ€‘2: Notebook contains markdown, code, and output in a clean sequence

ðŸ“˜ MODULE 1.2 â€” Spark DataFrames
Purpose
Learn Spark DataFrame basics: reading, transforming, writing.

Deliverables
sample_etl_demo.py

Reads sample datasets

Performs basic ETL

Writes Parquet + Delta

KPIs
KPIâ€‘3: Miniâ€‘ETL completed (raw â†’ cleaned â†’ aggregated)

KPIâ€‘4: df.explain() interpreted at a high level

ðŸ“˜ MODULE 1.3 â€” Your Own Delta Dataset
Purpose
Create your own dataset, write it as Delta, perform CRUD, and prepare for time travel.

Deliverables
01_generate_delta_dataset.py

02_transform_and_crud.py

README explaining dataset + pipeline

KPIs
KPIâ€‘5: One fully owned Delta table created

KPIâ€‘6: CRUD operations (insert/update/delete) executed successfully

KPIâ€‘7: Multiple versions created for time travel

KPIâ€‘8: Table registered and queryable

ðŸ“˜ KPIs_Level1.md (Full KPI List)
Module 1.1 KPIs
KPIâ€‘1: Notebook runs without cluster errors

KPIâ€‘2: Notebook includes markdown + code + results

Module 1.2 KPIs
KPIâ€‘3: Mini ETL pipeline completed

KPIâ€‘4: Explain plan interpreted

Module 1.3 KPIs
KPIâ€‘5: Custom Delta table created

KPIâ€‘6: CRUD operations validated

KPIâ€‘7: Time travel versions created

KPIâ€‘8: Table registered in metastore
